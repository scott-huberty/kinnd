{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import mffpy\n",
    "import kinnd\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = Path(\"/Volumes\") / \"UBUNTU18\" / \"USC\" / \"listen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory(directory):\n",
    "    \"\"\"Check if a directory exists, and raise an error if it doesn't.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str | Path\n",
    "        The directory to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The directory as a Path object.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If the directory does not exist.\n",
    "    IOError\n",
    "        If the directory is not a string or Path object.\n",
    "    \"\"\"\n",
    "    if not isinstance(directory, (str, Path)):\n",
    "        raise IOError(\n",
    "            f\"directory must be a string or Path object, not {type(directory)}\"\n",
    "            )\n",
    "    if not Path(directory).exists():\n",
    "        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n",
    "    return Path(directory)\n",
    "\n",
    "def read_raw_listen(filename):\n",
    "    import mffpy\n",
    "    import pytz\n",
    "    import datetime\n",
    "\n",
    "    mff_reader = mffpy.Reader(filename)\n",
    "    mff_reader.set_unit(\"EEG\", \"V\")\n",
    "\n",
    "    # Basic Information\n",
    "    meas_date = mff_reader.startdatetime\n",
    "    meas_date = meas_date.replace(tzinfo=pytz.timezone(\"US/Pacific\"))\n",
    "    meas_date = meas_date.astimezone(pytz.utc)\n",
    "    meas_date = meas_date.replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "    sfreq = mff_reader.sampling_rates[\"EEG\"]\n",
    "\n",
    "    # Montage\n",
    "    with mff_reader.directory.filepointer(\"info1\") as fp:\n",
    "        info = mffpy.XML.from_file(fp)\n",
    "    montage_map = {\"HydroCel GSN 128 1.0\": \"GSN-HydroCel-129\",}\n",
    "    mon = info.generalInformation[\"montageName\"]\n",
    "    montage = mne.channels.make_standard_montage(montage_map[mon])\n",
    "\n",
    "    # samples\n",
    "    eeg, _ = mff_reader.get_physical_samples()[\"EEG\"]\n",
    "\n",
    "    # Events\n",
    "    categories = mffpy.XML.from_file(filename / \"Events_ECI TCP-IP 55513.xml\")\n",
    "    events = categories.get_content()[\"event\"]\n",
    "\n",
    "\n",
    "    # Create MNE Objects\n",
    "    ch_names = montage.ch_names\n",
    "    ch_types = [\"eeg\"] * len(ch_names)\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    info.set_montage(montage)\n",
    "    info.set_meas_date(meas_date)\n",
    "    raw = mne.io.RawArray(eeg, info)\n",
    "\n",
    "    # Annotations\n",
    "    cel_map = {\"1\": \"match\", \"2\": \"mismatch\"}\n",
    "    WANT_EVENTS = [\"img+\", \"snd+\"]\n",
    "    stim_map = {\"img+\": \"image\", \"snd+\": \"word\"}\n",
    "\n",
    "    onsets = []\n",
    "    durations = []\n",
    "    descriptions = []\n",
    "    for event in events:\n",
    "        if event[\"code\"] not in WANT_EVENTS:\n",
    "            continue\n",
    "        onset = event[\"beginTime\"].replace(tzinfo=pytz.timezone(\"US/Pacific\"))\n",
    "        onset = onset.astimezone(pytz.utc).replace(tzinfo=datetime.timezone.utc)\n",
    "        ts = (onset - raw.info[\"meas_date\"]).total_seconds()\n",
    "        duration = event[\"duration\"] / 1000\n",
    "        condition = cel_map[str(event[\"keys\"][\"cel#\"])]\n",
    "        description = f\"{stim_map[event['code']]}_{condition}\"\n",
    "        onsets.append(ts)\n",
    "        durations.append(duration)\n",
    "        descriptions.append(description)\n",
    "    raw.set_annotations(mne.Annotations(onsets, durations, descriptions))\n",
    "    return raw\n",
    "\n",
    "def get_listen_fpaths(directory=None):\n",
    "    \"\"\"Get the filepaths for Listen EEG data.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    if directory is None:\n",
    "        raise NotImplementedError(\"Automatic search for Listen files not implemented yet.\")\n",
    "    directory = check_directory(directory)\n",
    "    fpaths = kinnd.paths.get_eeg_fpaths(study=\"listen\", directory=directory)\n",
    "    subject_dict = defaultdict(dict)\n",
    "\n",
    "    for fpath in fpaths:\n",
    "        fname = fpath.name.lower()\n",
    "        key = f\"sub-{fname.split('_')[1]}\"\n",
    "        if \"phonemes\" in fname:\n",
    "            subject_dict[key][\"phonemes\"] = fpath\n",
    "        elif \"resting\" in fname:\n",
    "            subject_dict[key][\"resting\"] = fpath\n",
    "        elif \"semantics\" in fname:\n",
    "            subject_dict[key][\"semantics\"] = fpath\n",
    "        else:\n",
    "            raise ValueError(f\"File {fpath} does not match appear to belong to a Listen task.\"\n",
    "                             \"Expected 'phonemes', 'resting', or 'semantics' in the filename.\")\n",
    "    return dict(subject_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listen_fpaths = get_listen_fpaths(directory)\n",
    "listen_fpaths.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylossless as ll \n",
    "\n",
    "config = ll.config.Config()\n",
    "config.load_default()\n",
    "config_fpath = Path(\".\") / \"listen_config.yaml\"\n",
    "\n",
    "config[\"filtering\"][\"notch_filter_args\"][\"freqs\"] = [60]\n",
    "config[\"noisy_channels\"][\"outliers_kwargs\"][\"k\"] = 6\n",
    "config[\"noisy_channels\"][\"outliers_kwargs\"][\"lower\"] = 0.25\n",
    "config[\"noisy_channels\"][\"outliers_kwargs\"][\"upper\"] = 0.75\n",
    "\n",
    "config[\"noisy_epochs\"][\"outliers_kwargs\"][\"k\"] = 6\n",
    "config[\"noisy_epochs\"][\"outliers_kwargs\"][\"lower\"] = 0.25\n",
    "config[\"noisy_epochs\"][\"outliers_kwargs\"][\"upper\"] = 0.75\n",
    "\n",
    "config[\"uncorrelated_channels\"][\"outliers_kwargs\"][\"k\"] = 6\n",
    "config[\"uncorrelated_channels\"][\"outliers_kwargs\"][\"lower\"] = 0.25\n",
    "config[\"uncorrelated_channels\"][\"outliers_kwargs\"][\"upper\"] = 0.75\n",
    "\n",
    "config[\"uncorrelated_epochs\"][\"outliers_kwargs\"][\"k\"] = 6\n",
    "config[\"uncorrelated_epochs\"][\"outliers_kwargs\"][\"lower\"] = 0.25\n",
    "config[\"uncorrelated_epochs\"][\"outliers_kwargs\"][\"upper\"] = 0.75\n",
    "\n",
    "config[\"ica\"][\"noisy_ic_epochs\"][\"outliers_kwargs\"][\"k\"] = 6\n",
    "config[\"ica\"][\"noisy_ic_epochs\"][\"outliers_kwargs\"][\"lower\"] = 0.25\n",
    "config[\"ica\"][\"noisy_ic_epochs\"][\"outliers_kwargs\"][\"upper\"] = 0.75\n",
    "\n",
    "config.save(config_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for subject in tqdm(listen_fpaths, total=len(listen_fpaths)):\n",
    "    try:\n",
    "        semantics_fpath = listen_fpaths[subject][\"semantics\"]\n",
    "    except KeyError:\n",
    "        print(f\"### Skipping {subject} because no semantics file found.\")\n",
    "        continue\n",
    "    if (directory / \"derivatives\" / \"pylossless\" / subject).exists():\n",
    "        print(f\"### Skipping {subject} because output already exists.\")\n",
    "        continue\n",
    "    print(f\"### Processing {subject} ###\")\n",
    "    raw = read_raw_listen(semantics_fpath)\n",
    "    raw.info[\"bads\"].extend([\"E125\", \"E126\", \"E127\", \"E128\"])\n",
    "\n",
    "    # Find Breaks\n",
    "    break_annots = mne.preprocessing.annotate_break(raw)\n",
    "    raw.set_annotations(raw.annotations + break_annots)\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = ll.LosslessPipeline(config_fpath)\n",
    "    pipeline.run_with_raw(raw)\n",
    "    rejection_policy = ll.RejectionPolicy()\n",
    "    cleaned_raw = rejection_policy.apply(pipeline)\n",
    "\n",
    "    dpath = directory / \"derivatives\" / \"pylossless\" / subject\n",
    "    dpath.mkdir(exist_ok=False, parents=False)\n",
    "    eeg_out = dpath / f\"{subject}_ses-01_task-semantics_eeg.fif\"\n",
    "    cleaned_raw.save(eeg_out)\n",
    "    ica_out = dpath / f\"{subject}_ses-01_task-semantics_ica.fif\"\n",
    "    pipeline.ica2.save(ica_out)\n",
    "    labels_out = dpath / f\"{subject}_ses-01_task-semantics_ic-labels.csv\"\n",
    "    pipeline.flags[\"ic\"].to_csv(labels_out)\n",
    "    del cleaned_raw, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed Raws, Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "def load_eeg(fpath):\n",
    "    return mne.io.read_raw(fpath)\n",
    "\n",
    "def make_epochs(raw, tmin=-0.1, tmax=2, preload=False):\n",
    "    events, event_ids = mne.events_from_annotations(\n",
    "        raw,\n",
    "        regexp=\"image_match|image_mismatch\",\n",
    "        event_id={\"image_match\": 1, \"image_mismatch\": 2}\n",
    "        )\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events,\n",
    "        event_id={\"match\": 1, \"mismatch\": 2},\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        preload=preload\n",
    "        )\n",
    "    return epochs\n",
    "\n",
    "\n",
    "def get_evoked(epochs, event_id=None):\n",
    "    if event_id is not None:\n",
    "        epochs = epochs[event_id]\n",
    "    evoked = epochs.apply_baseline((None, 0)).average()\n",
    "    return evoked\n",
    "\n",
    "\n",
    "def epochs_to_xr(epochs):\n",
    "    data = epochs.get_data()\n",
    "    coords = {\n",
    "        \"epoch\": range(len(epochs)),\n",
    "        \"time\": epochs.times,\n",
    "        \"channel\": epochs.ch_names,\n",
    "        }\n",
    "    dims = (\"epoch\", \"channel\", \"time\")\n",
    "    return xr.DataArray(data, coords=coords, dims=dims)\n",
    "\n",
    "\n",
    "def evoked_to_xr(evoked, subject):\n",
    "    data = [evoked.data]\n",
    "    coords = {\n",
    "        \"subject\": [subject],\n",
    "        \"channel\": evoked.ch_names,\n",
    "        \"time\": evoked.times,\n",
    "        }\n",
    "    dims = (\"subject\", \"channel\", \"time\")\n",
    "    return xr.DataArray(data, coords=coords, dims=dims)\n",
    "\n",
    "\n",
    "def get_evoked_xr(evoked, subject):\n",
    "    dims = (\"subject\", \"channel\", \"time\")\n",
    "    coords = {\"subject\": [subject], \"channel\": evoked.ch_names, \"time\": evoked.times}\n",
    "    data = [evoked.data]\n",
    "    return xr.DataArray(data, coords=coords, dims=dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIRT = [\n",
    "    \"E81\", \"E88\", \"E94\", \"E99\", \"E107\", \"E113\", \"E119\", \"E120\", \"E121\", \"E125\",\n",
    "    \"E126\", \"E8\", \"E9\", \"E14\", \"E17\", \"E21\", \"E22\", \"E25\", \"E127\", \"E32\", \"E128\",\n",
    "    \"E43\", \"E48\", \"E49\", \"E56\", \"E63\", \"E68\", \"E73\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot(theme=\"light\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives_dir = directory / \"derivatives\" / \"pylossless\"\n",
    "\n",
    "derivative_paths = list(derivatives_dir.rglob(\"sub-*_eeg.fif\"))\n",
    "subjects = [p.parent.name for p in derivative_paths]\n",
    "\n",
    "\n",
    "derivative_paths = list(derivatives_dir.rglob(\"sub-*_eeg.fif\"))\n",
    "subjects = [p.parent.name for p in derivative_paths]\n",
    "\n",
    "SAVE_EPOCHS = True\n",
    "evoked_da = []\n",
    "raws = dict()\n",
    "for file, subject in list(zip(derivative_paths, subjects)):\n",
    "    raw = load_eeg(file).load_data()\n",
    "    raw.filter(None, 50)\n",
    "    raw.interpolate_bads(reset_bads=False)\n",
    "    raw.set_eeg_reference()\n",
    "    raws[subject] = raw.copy()\n",
    "    epochs = make_epochs(raw, preload=True)\n",
    "    if SAVE_EPOCHS:\n",
    "        out_path = file.parent.parent.parent / \"epochs\" / f\"{file.stem}_epo.fif\"\n",
    "        epochs.save(out_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# epo_fpaths = kinnd.paths.get_epo_fpaths(study=\"listen\", directory=directory)\n",
    "epo_dir = directory / \"derivatives\" / \"epochs\"\n",
    "epo_fpaths = list(epo_dir.glob(\"*_epo.fif\"))\n",
    "subjects = [f.stem.split(\"_\")[0] for f in epo_fpaths]\n",
    "\n",
    "n_events = {\"match\": [], \"mismatch\": []}\n",
    "n_bad_channels = []\n",
    "\n",
    "epos = dict()\n",
    "for epo_fpath, subject in tqdm(list(zip(epo_fpaths, subjects)), total=len(epo_fpaths)):\n",
    "    ep = mne.read_epochs(epo_fpath)\n",
    "    n_events[\"match\"].append(len(ep[\"match\"]))\n",
    "    n_events[\"mismatch\"].append(len(ep[\"mismatch\"]))\n",
    "    n_bad_channels.append(len(ep.info[\"bads\"]))\n",
    "    epos[subject] = ep.copy()\n",
    "    del ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "colors = sns.color_palette()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.violinplot(data=n_events, ax=axes[0])\n",
    "sns.stripplot(data=n_events, ax=axes[0], color=colors[4])\n",
    "axes[0].set_title(\"Number of Events per Condition\")\n",
    "axes[0].set_ylabel(\"Number of Events\")\n",
    "axes[0].set_xticklabels([\"Match\", \"Mismatch\"])\n",
    "axes[0].axhline(60, color=\"red\", linestyle=\"--\", label=\"Expected Events\")\n",
    "\n",
    "NUM_CHANNELS = 129\n",
    "num_good_channels = [NUM_CHANNELS - n for n in n_bad_channels]\n",
    "sns.violinplot(data=num_good_channels, ax=axes[1])\n",
    "sns.stripplot(data=num_good_channels, ax=axes[1], color=colors[4])\n",
    "axes[1].set_title(\"Number of Good Channels per Subject\")\n",
    "axes[1].set_ylabel(\"Number of Good Channels\")\n",
    "axes[1].axhline(129, color=\"red\", linestyle=\"--\", label=\"Total Channels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject, epo in tqdm(epos.items(), total=len(epos)):\n",
    "    evoked_ds = xr.Dataset(\n",
    "        {\"match\": get_evoked_xr(get_evoked(epo[\"match\"]), subject),\n",
    "        \"mismatch\": get_evoked_xr(get_evoked(epo[\"mismatch\"]), subject)}\n",
    "        )\n",
    "    evoked_da.append(evoked_ds)\n",
    "evoked_semantics = xr.concat(evoked_da, dim=\"subject\")\n",
    "evoked_semantics.to_netcdf(directory / \"derivatives\" / \"evoked.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 6))\n",
    "\n",
    "info = epochs.info\n",
    "ch_names = evoked_semantics.coords[\"channel\"].values\n",
    "topo_kwargs = dict(vlim=(-.000025, .000025), show=False, names=ch_names)\n",
    "\n",
    "data_dict = dict(\n",
    "    zip(\n",
    "        ch_names,\n",
    "        evoked_semantics[\"match\"].sel(time=slice(1, None)).mean([\"subject\", \"time\"]).values)\n",
    "    )\n",
    "im = kinnd.viz.plot_topomap(data_dict, info, axes=ax[0],  **topo_kwargs)\n",
    "ax[0].set_title(\"Match\")\n",
    "\n",
    "data_dict = dict(\n",
    "    zip(ch_names,\n",
    "    evoked_semantics[\"mismatch\"].sel(time=slice(1, None)).mean([\"subject\", \"time\"]).values)\n",
    "    )\n",
    "im = kinnd.viz.plot_topomap(data_dict, info, axes=ax[1], **topo_kwargs)\n",
    "ax[1].set_title(\"Mismatch\")\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.5, cmap=\"RdBu_r\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = [\"E29\", \"E13\", \"E6\", \"E112\", \"E111\", \"E28\", \"E20\", \"E12\", \"E5\", \"E118\", \"E39\", \"E7\", \"E106\", \"E105\"]\n",
    "\n",
    "fig, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "df = evoked_semantics.sel(channel=ROI).mean(\"channel\").to_dataframe().reset_index()\n",
    "df = df.melt(id_vars=[\"subject\", \"time\"], var_name=\"condition\", value_name=\"amplitude\")\n",
    "sns.lineplot(data=df, x=\"time\", y=\"amplitude\", hue=\"condition\", ax=ax, n_boot=1000)\n",
    "\n",
    "ax.axvspan(0, 1, color=colors[4], alpha=0.2)\n",
    "ax.text(0.5, 0, \"Image\", fontsize=14)\n",
    "\n",
    "ax.axvline(1, color=colors[5], linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd_to_xr(psd, subject):\n",
    "    data = [psd.get_data()]\n",
    "    coords = {\n",
    "        \"subject\": [subject],\n",
    "        \"channel\": psd.ch_names,\n",
    "        \"freq\": psd.freqs,\n",
    "        }\n",
    "    dims = (\"subject\", \"channel\", \"freq\")\n",
    "    return xr.DataArray(data, coords=coords, dims=dims)\n",
    "\n",
    "psd_da = []\n",
    "for subject, epo in tqdm(epos.items(), total=len(epos)):\n",
    "    epo.info[\"bads\"] = []\n",
    "    psd = epo.compute_psd(method=\"welch\", fmin=1, fmax=50)\n",
    "    this_ds = xr.Dataset(\n",
    "        {\"match\": psd_to_xr(psd[\"match\"].average(), subject),\n",
    "        \"mismatch\": psd_to_xr(psd[\"mismatch\"].average(), subject)\n",
    "        }\n",
    "    )\n",
    "    psd_da.append(this_ds)\n",
    "psd_semantics = xr.concat(psd_da, dim=\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def plot_psd(semantics_psd, ax, condition=\"match\"):\n",
    "\n",
    "    fig = ax.get_figure()\n",
    "\n",
    "    df = semantics_psd.median(\"subject\").to_dataframe().reset_index()\n",
    "    df = df.melt(id_vars=[\"channel\", \"freq\"], var_name=\"condition\", value_name=\"power\")\n",
    "    df = df.loc[df[\"condition\"] == condition]\n",
    "    df[\"power\"] = np.log10(df[\"power\"])\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x=\"freq\",\n",
    "        y=\"power\",\n",
    "        hue=\"channel\",\n",
    "        ax=ax,\n",
    "        palette=[sns.color_palette()[0]],\n",
    "        alpha=0.5,\n",
    "        linewidth=.5,\n",
    "        n_boot=100,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    df_grand_av = psd_semantics.median([\"subject\", \"channel\"]).to_dataframe().reset_index()\n",
    "    df_grand_av = df_grand_av.melt(id_vars=[\"freq\"], var_name=\"condition\", value_name=\"power\")\n",
    "    df_grand_av = df_grand_av.loc[df_grand_av[\"condition\"] == condition]\n",
    "    df_grand_av[\"power\"] = np.log10(df_grand_av[\"power\"])\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=df_grand_av,\n",
    "        x=\"freq\",\n",
    "        y=\"power\",\n",
    "        ax=ax,\n",
    "        color=sns.color_palette(\"tab10\")[1],\n",
    "        linewidth=2,\n",
    "        errorbar=None,\n",
    "    )\n",
    "    ax.set_title(condition)\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax.set_ylabel(\"Power (log10)\")\n",
    "    ax.set_xlim(0, 50)\n",
    "    return ax\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "plot_psd(psd_semantics, ax[0], \"match\")\n",
    "plot_psd(psd_semantics, ax[1], \"mismatch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "mne.viz.plot_topomap(list(data_dict.values()), info, show=False, names=ch_names, axes=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude drop_chs\n",
    "psd_semantics.sel(channel=~np.isin(psd_semantics[\"channel\"], SKIRT)).coords[\"channel\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# vlim = np.quantile(list(data_dict.values()), [.05, .95])\n",
    "topo_kwargs = dict(vlim=vlim, show=False)\n",
    "topo_kwargs = dict(show=False)\n",
    "drop_chs = [\"E125\", \"E126\", \"E127\", \"E128\"]\n",
    "SUBSET = ~np.isin(psd_semantics[\"channel\"], SKIRT)\n",
    "\n",
    "info = epochs.copy().drop_channels(SKIRT).info\n",
    "\n",
    "ch_names = psd_semantics.sel(channel=SUBSET).coords[\"channel\"].values\n",
    "data_dict = dict(\n",
    "    zip(\n",
    "        ch_names,\n",
    "        psd_semantics[\"match\"].sel(channel=SUBSET, freq=slice(7, 11)).median([\"subject\"]).mean([\"freq\"]).values)\n",
    "    )\n",
    "vlim = np.quantile(list(data_dict.values()), [.05, .95])\n",
    "im = kinnd.viz.plot_topomap(data_dict, info, axes=ax[0], **topo_kwargs)\n",
    "ax[0].set_title(\"Match\")\n",
    "\n",
    "data_dict = dict(\n",
    "    zip(\n",
    "        ch_names,\n",
    "        psd_semantics[\"mismatch\"].sel(channel=SUBSET, freq=slice(7, 11)).median([\"subject\"]).mean([\"freq\"]).values)\n",
    "    )\n",
    "\n",
    "im = kinnd.viz.plot_topomap(data_dict, info, axes=ax[1], **topo_kwargs)\n",
    "ax[1].set_title(\"Mismatch\")\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.5, cmap=\"RdBu_r\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinnd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
